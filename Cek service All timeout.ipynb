import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook
import time
import datetime
import pytz

# Function to check if the response time is more than 10 seconds
def response_time_exceeds_limit(url, limit=10):
    try:
        start_time = time.time()
        response = requests.get(url, timeout=limit)
        response.raise_for_status()
        elapsed_time = time.time() - start_time
        print(f"sedang mengecek {url}")
        return elapsed_time > limit
    except requests.RequestException as e:
        print(f"Failed to fetch {url}: {e}")
        return True  # Consider it as exceeding limit if there's an exception

# Function to get all hyperlinks from a webpage
def get_hyperlinks(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        return [a['href'] for a in soup.find_all('a', href=True)]
    except requests.RequestException as e:
        print(f"Failed to fetch {url}: {e}")
        return []

# Function to check hyperlinks and nested hyperlinks, then write results to an Excel file
def check_links_and_write_to_excel(start_url):
    # Get the initial list of hyperlinks
    initial_links = get_hyperlinks(start_url)
    initial_links = list(set(initial_links))  # Remove duplicate links
    
    # Initialize Excel workbook
    wb = Workbook()
    ws = wb.active
    ws.append(["Link Address", "Response Time > 10 Seconds"])

    # Check the response time for each initial link
    for link in initial_links:
        if not link.startswith('http'):
            link = requests.compat.urljoin(start_url, link)
        result = response_time_exceeds_limit(link)
        ws.append([link, "Yes" if result else "No"])

        # Get hyperlinks from each page in the initial list and check their response time
        nested_links = get_hyperlinks(link)
        nested_links = list(set(nested_links))  # Remove duplicate links
        for nested_link in nested_links:
            if not nested_link.startswith('http'):
                nested_link = requests.compat.urljoin(link, nested_link)
            nested_result = response_time_exceeds_limit(nested_link)
            ws.append([nested_link, "Yes" if nested_result else "No"])

    # Get current date and time
    tz = pytz.timezone('Asia/Jakarta')
    current_datetime = datetime.datetime.now(tz)
    date_string = current_datetime.strftime("%Y%m%d")
    time_string = current_datetime.strftime("%H%M")  # 24-hour format

    # Create file name with date and time
    file_name = f"D:\\Downloads\\link_check_timeout_{date_string}_{time_string}.xlsx"    
    
    # Save the Excel file
    wb.save(file_name)
    print("proses berhasil")

# Example usage
start_url = "https://gis.bnpb.go.id/server/rest/services"  # Replace this with your starting webpage URL
check_links_and_write_to_excel(start_url)
