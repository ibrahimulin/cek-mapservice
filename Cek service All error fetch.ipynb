{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b27e1bc-1960-4c31-acb6-34d17a15c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch https://gis.bnpb.go.id/server/rest/services/thematic/20240511_022824_75_24d1_3B_AnalyticMS_SR_enhc_tile/ImageServer: 500 Server Error: Internal Server Error for url: https://gis.bnpb.go.id/server/rest/services/thematic/20240511_022824_75_24d1_3B_AnalyticMS_SR_enhc_tile/ImageServer\n",
      "Process completed successfully :)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "# Function to check if the word \"administrator\" is in the page content\n",
    "def contains_administrator(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return \"administrator\" in response.text.lower()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch {url}: {e}\")\n",
    "        return f\"Failed: {e}\"\n",
    "\n",
    "# Function to get all hyperlinks from a webpage\n",
    "def get_hyperlinks(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to check hyperlinks and nested hyperlinks, then write results to an Excel file\n",
    "def check_links_and_write_to_excel(start_url):\n",
    "    # Get the initial list of hyperlinks\n",
    "    initial_links = get_hyperlinks(start_url)\n",
    "    initial_links = list(set(initial_links))  # Remove duplicate links\n",
    "    \n",
    "    # Initialize Excel workbook\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append([\"Link Address\", \"Contains 'Administrator'\", \"Error Message\"])\n",
    "\n",
    "    # Check the response time for each initial link\n",
    "    for link in initial_links:\n",
    "        if not link.startswith('http'):\n",
    "            link = requests.compat.urljoin(start_url, link)\n",
    "        result = contains_administrator(link)\n",
    "        if isinstance(result, str) and result.startswith(\"Failed\"):\n",
    "            ws.append([link, \"No\", result])  # Record the failure message\n",
    "        else:\n",
    "            ws.append([link, \"Yes\" if result else \"No\", \"\"])\n",
    "\n",
    "        # Get hyperlinks from each page in the initial list and check their response time\n",
    "        nested_links = get_hyperlinks(link)\n",
    "        nested_links = list(set(nested_links))  # Remove duplicate links\n",
    "        for nested_link in nested_links:\n",
    "            if not nested_link.startswith('http'):\n",
    "                nested_link = requests.compat.urljoin(link, nested_link)\n",
    "            nested_result = contains_administrator(nested_link)\n",
    "            if isinstance(nested_result, str) and nested_result.startswith(\"Failed\"):\n",
    "                ws.append([nested_link, \"No\", nested_result])  # Record the failure message\n",
    "            else:\n",
    "                ws.append([nested_link, \"Yes\" if nested_result else \"No\", \"\"])\n",
    "\n",
    "    # Get current date and time\n",
    "    tz = pytz.timezone('Asia/Jakarta')\n",
    "    current_datetime = datetime.datetime.now(tz)\n",
    "    date_string = current_datetime.strftime(\"%Y%m%d\")\n",
    "    time_string = current_datetime.strftime(\"%H%M\")  # 24-hour format\n",
    "\n",
    "    # Create file name with date and time\n",
    "    file_name = f\"D:\\\\Downloads\\\\link_check_errorfetch_{date_string}_{time_string}.xlsx\"\n",
    "    \n",
    "    wb.save(file_name)\n",
    "    print(\"Process completed successfully :)\")\n",
    "\n",
    "# Example usage\n",
    "start_url = \"https://gis.bnpb.go.id/server/rest/services/\"  # Replace this with your starting webpage URL\n",
    "check_links_and_write_to_excel(start_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111013f-54eb-4431-bb54-c4755d7d905f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
